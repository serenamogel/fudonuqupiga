<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>SnapJolt | Cerebras' 1.2 Trillion Transistor Deep Learning Processor</title><meta name=description content="08:49PM EDT - Some of the big news of today is Cerebras announcing its wafer-scale 1.2 trillion transistor solution for deep learning. The talk today goes into detail about the technology. 08:51PM EDT - Wafer scale chip, over 46,225 mm2, 1.2 trillion transistors, 400k AI cores, fed by 18GB of on-chip SRAM"><meta name=robots content="index,follow,noarchive"><meta name=author content="Martina Birk"><link rel=icon href=./favicon.ico><link rel=stylesheet href=https://assets.cdnweb.info/hugo/sk3/css/sk3.css><style>@media only screen and (min-width:993px){:root{--width-card:32%}}@media only screen and (min-width:993px){:root{--width-html:1200px}}:root{--color-bg:#181a1b}:root{--color-border:white}:root{--color-link:#3d84ff}:root{--color-txt:white}</style><link rel=stylesheet href=./css/abc.css><link rel=stylesheet href=./css/def.css><script>enableOpenToc=!1</script><script defer src=https://assets.cdnweb.info/hugo/sk3/js/sk3.js></script>
<script src=./js/abc.js></script>
<script src=./js/def.js></script>
<script></script><svg xmlns="http://www.w3.org/2000/svg" style="display:none"><symbol id="facebook" viewBox="0 0 512 512"><path d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14.0 55.52 4.84 55.52 4.84v61h-31.28c-30.8.0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></symbol><symbol id="flickr" viewBox="0 0 448 512"><path d="M4e2 32H48C21.5 32 0 53.5.0 80v352c0 26.5 21.5 48 48 48h352c26.5.0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zM144.5 319c-35.1.0-63.5-28.4-63.5-63.5s28.4-63.5 63.5-63.5 63.5 28.4 63.5 63.5-28.4 63.5-63.5 63.5zm159 0c-35.1.0-63.5-28.4-63.5-63.5s28.4-63.5 63.5-63.5 63.5 28.4 63.5 63.5-28.4 63.5-63.5 63.5z"/></symbol><symbol id="github" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></symbol><symbol id="instagram" viewBox="0 0 448 512"><path d="M224.1 141c-63.6.0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1.0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9.0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9.0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9.0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9.0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8.0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"/></symbol><symbol id="linkedin" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></symbol><symbol id="pinterest" viewBox="0 0 496 512"><path d="M496 256c0 137-111 248-248 248-25.6.0-50.2-3.9-73.4-11.1 10.1-16.5 25.2-43.5 30.8-65 3-11.6 15.4-59 15.4-59 8.1 15.4 31.7 28.5 56.8 28.5 74.8.0 128.7-68.8 128.7-154.3.0-81.9-66.9-143.2-152.9-143.2-107 0-163.9 71.8-163.9 150.1.0 36.4 19.4 81.7 50.3 96.1 4.7 2.2 7.2 1.2 8.3-3.3.8-3.4 5-20.3 6.9-28.1.6-2.5.3-4.7-1.7-7.1-10.1-12.5-18.3-35.3-18.3-56.6.0-54.7 41.4-107.6 112-107.6 60.9.0 103.6 41.5 103.6 100.9.0 67.1-33.9 113.6-78 113.6-24.3.0-42.6-20.1-36.7-44.8 7-29.5 20.5-61.3 20.5-82.6.0-19-10.2-34.9-31.4-34.9-24.9.0-44.9 25.7-44.9 60.2.0 22 7.4 36.8 7.4 36.8s-24.5 103.8-29 123.2c-5 21.4-3 51.6-.9 71.2C65.4 450.9.0 361.1.0 256 0 119 111 8 248 8s248 111 248 248z"/></symbol><symbol id="qq" viewBox="0 0 448 512"><path d="M433.754 420.445c-11.526 1.393-44.86-52.741-44.86-52.741.0 31.345-16.136 72.247-51.051 101.786 16.842 5.192 54.843 19.167 45.803 34.421-7.316 12.343-125.51 7.881-159.632 4.037-34.122 3.844-152.316 8.306-159.632-4.037-9.045-15.25 28.918-29.214 45.783-34.415-34.92-29.539-51.059-70.445-51.059-101.792.0.0-33.334 54.134-44.859 52.741-5.37-.65-12.424-29.644 9.347-99.704 10.261-33.024 21.995-60.478 40.144-105.779C60.683 98.063 108.982.006 224 0c113.737.006 163.156 96.133 160.264 214.963 18.118 45.223 29.912 72.85 40.144 105.778 21.768 70.06 14.716 99.053 9.346 99.704z"/></symbol><symbol id="reddit" viewBox="0 0 512 512"><path d="M201.5 305.5c-13.8.0-24.9-11.1-24.9-24.6.0-13.8 11.1-24.9 24.9-24.9 13.6.0 24.6 11.1 24.6 24.9.0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4.0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7.0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9.0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5.0 52.6 59.2 95.2 132 95.2 73.1.0 132.3-42.6 132.3-95.2.0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6.0-2.2-2.2-6.1-2.2-8.3.0-2.5 2.5-2.5 6.4.0 8.6 22.8 22.8 87.3 22.8 110.2.0 2.5-2.2 2.5-6.1.0-8.6-2.2-2.2-6.1-2.2-8.3.0zm7.7-75c-13.6.0-24.6 11.1-24.6 24.9.0 13.6 11.1 24.6 24.6 24.6 13.8.0 24.9-11.1 24.9-24.6.0-13.8-11-24.9-24.9-24.9z"/></symbol><symbol id="stack-exchange" viewBox="0 0 448 512"><path d="M17.7 332.3h412.7v22c0 37.7-29.3 68-65.3 68h-19L259.3 512v-89.7H83c-36 0-65.3-30.3-65.3-68v-22zm0-23.6h412.7v-85H17.7v85zm0-109.4h412.7v-85H17.7v85zM365 0H83C47 0 17.7 30.3 17.7 67.7V90h412.7V67.7C430.3 30.3 401 0 365 0z"/></symbol><symbol id="stack-overflow" viewBox="0 0 384 512"><path d="M290.7 311 95 269.7 86.8 309l195.7 41zm51-87L188.2 95.7l-25.5 30.8 153.5 128.3zm-31.2 39.7L129.2 179l-16.7 36.5L293.7 3e2zM262 32l-32 24 119.3 160.3 32-24zm20.5 328h-2e2v39.7h2e2zm39.7 80H42.7V320h-40v160h359.5V320h-40z"/></symbol><symbol id="telegram" viewBox="0 0 496 512"><path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm121.8 169.9-40.7 191.8c-3 13.6-11.1 16.9-22.4 10.5l-62-45.7-29.9 28.8c-3.3 3.3-6.1 6.1-12.5 6.1l4.4-63.1 114.9-103.8c5-4.4-1.1-6.9-7.7-2.5l-142 89.4-61.2-19.1c-13.3-4.2-13.6-13.3 2.8-19.7l239.1-92.2c11.1-4 20.8 2.7 17.2 19.5z"/></symbol><symbol id="tumblr" viewBox="0 0 320 512"><path d="M309.8 480.3c-13.6 14.5-50 31.7-97.4 31.7-120.8.0-147-88.8-147-140.6v-144H17.9c-5.5.0-10-4.5-10-10v-68c0-7.2 4.5-13.6 11.3-16 62-21.8 81.5-76 84.3-117.1.8-11 6.5-16.3 16.1-16.3h70.9c5.5.0 10 4.5 10 10v115.2h83c5.5.0 10 4.4 10 9.9v81.7c0 5.5-4.5 10-10 10h-83.4V360c0 34.2 23.7 53.6 68 35.8 4.8-1.9 9-3.2 12.7-2.2 3.5.9 5.8 3.4 7.4 7.9l22 64.3c1.8 5 3.3 10.6-.4 14.5z"/></symbol><symbol id="twitch" viewBox="0 0 512 512"><path d="M391.17 103.47H352.54v109.7h38.63zM285 103H246.37V212.75H285zM120.83.0 24.31 91.42V420.58H140.14V512l96.53-91.42h77.25L487.69 256V0zM449.07 237.75l-77.22 73.12H294.61l-67.6 64v-64H140.14V36.58H449.07z"/></symbol><symbol id="twitter" viewBox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></symbol><symbol id="vimeo" viewBox="0 0 448 512"><path d="M403.2 32H44.8C20.1 32 0 52.1.0 76.8v358.4C0 459.9 20.1 480 44.8 480h358.4c24.7.0 44.8-20.1 44.8-44.8V76.8c0-24.7-20.1-44.8-44.8-44.8zM377 180.8c-1.4 31.5-23.4 74.7-66 129.4-44 57.2-81.3 85.8-111.7 85.8-18.9.0-34.8-17.4-47.9-52.3-25.5-93.3-36.4-148-57.4-148-2.4.0-10.9 5.1-25.4 15.2l-15.2-19.6c37.3-32.8 72.9-69.2 95.2-71.2 25.2-2.4 40.7 14.8 46.5 51.7 20.7 131.2 29.9 151 67.6 91.6 13.5-21.4 20.8-37.7 21.8-48.9 3.5-33.2-25.9-30.9-45.8-22.4 15.9-52.1 46.3-77.4 91.2-76 33.3.9 49 22.5 47.1 64.7z"/></symbol><symbol id="vk" viewBox="0 0 576 512"><path d="M545 117.7c3.7-12.5.0-21.7-17.8-21.7h-58.9c-15 0-21.9 7.9-25.6 16.7.0.0-30 73.1-72.4 120.5-13.7 13.7-20 18.1-27.5 18.1-3.7.0-9.4-4.4-9.4-16.9V117.7c0-15-4.2-21.7-16.6-21.7h-92.6c-9.4.0-15 7-15 13.5.0 14.2 21.2 17.5 23.4 57.5v86.8c0 19-3.4 22.5-10.9 22.5-20 0-68.6-73.4-97.4-157.4-5.8-16.3-11.5-22.9-26.6-22.9H38.8c-16.8.0-20.2 7.9-20.2 16.7.0 15.6 20 93.1 93.1 195.5C160.4 378.1 229 416 291.4 416c37.5.0 42.1-8.4 42.1-22.9.0-66.8-3.4-73.1 15.4-73.1 8.7.0 23.7 4.4 58.7 38.1 40 40 46.6 57.9 69 57.9h58.9c16.8.0 25.3-8.4 20.4-25-11.2-34.9-86.9-106.7-90.3-111.5-8.7-11.2-6.2-16.2.0-26.2.1-.1 72-101.3 79.4-135.6z"/></symbol><symbol id="weibo" viewBox="0 0 512 512"><path d="M407 177.6c7.6-24-13.4-46.8-37.4-41.7-22 4.8-28.8-28.1-7.1-32.8 50.1-10.9 92.3 37.1 76.5 84.8-6.8 21.2-38.8 10.8-32-10.3zM214.8 446.7C108.5 446.7.0 395.3.0 310.4c0-44.3 28-95.4 76.3-143.7C176 67 279.5 65.8 249.9 161c-4 13.1 12.3 5.7 12.3 6 79.5-33.6 140.5-16.8 114 51.4-3.7 9.4 1.1 10.9 8.3 13.1 135.7 42.3 34.8 215.2-169.7 215.2zm143.7-146.3c-5.4-55.7-78.5-94-163.4-85.7-84.8 8.6-148.8 60.3-143.4 116s78.5 94 163.4 85.7c84.8-8.6 148.8-60.3 143.4-116zM347.9 35.1c-25.9 5.6-16.8 43.7 8.3 38.3 72.3-15.2 134.8 52.8 111.7 124-7.4 24.2 29.1 37 37.4 12 31.9-99.8-55.1-195.9-157.4-174.3zm-78.5 311c-17.1 38.8-66.8 60-109.1 46.3-40.8-13.1-58-53.4-40.3-89.7 17.7-35.4 63.1-55.4 103.4-45.1 42 10.8 63.1 50.2 46 88.5zm-86.3-30c-12.9-5.4-30 .3-38 12.9-8.3 12.9-4.3 28 8.6 34 13.1 6 30.8.3 39.1-12.9 8-13.1 3.7-28.3-9.7-34zm32.6-13.4c-5.1-1.7-11.4.6-14.3 5.4-2.9 5.1-1.4 10.6 3.7 12.9 5.1 2 11.7-.3 14.6-5.4 2.8-5.2 1.1-10.9-4-12.9z"/></symbol><symbol id="weixin" viewBox="0 0 576 512"><path d="M385.2 167.6c6.4.0 12.6.3 18.8 1.1C387.4 90.3 303.3 32 207.7 32 100.5 32 13 104.8 13 197.4c0 53.4 29.3 97.5 77.9 131.6l-19.3 58.6 68-34.1c24.4 4.8 43.8 9.7 68.2 9.7 6.2.0 12.1-.3 18.3-.8-4-12.9-6.2-26.6-6.2-40.8-.1-84.9 72.9-154 165.3-154zm-104.5-52.9c14.5.0 24.2 9.7 24.2 24.4.0 14.5-9.7 24.2-24.2 24.2-14.8.0-29.3-9.7-29.3-24.2.1-14.7 14.6-24.4 29.3-24.4zm-136.4 48.6c-14.5.0-29.3-9.7-29.3-24.2.0-14.8 14.8-24.4 29.3-24.4 14.8.0 24.4 9.7 24.4 24.4.0 14.6-9.6 24.2-24.4 24.2zM563 319.4c0-77.9-77.9-141.3-165.4-141.3-92.7.0-165.4 63.4-165.4 141.3S305 460.7 397.6 460.7c19.3.0 38.9-5.1 58.6-9.9l53.4 29.3-14.8-48.6C534 402.1 563 363.2 563 319.4zm-219.1-24.5c-9.7.0-19.3-9.7-19.3-19.6.0-9.7 9.7-19.3 19.3-19.3 14.8.0 24.4 9.7 24.4 19.3.0 10-9.7 19.6-24.4 19.6zm107.1.0c-9.7.0-19.3-9.7-19.3-19.6.0-9.7 9.7-19.3 19.3-19.3 14.5.0 24.4 9.7 24.4 19.3.1 10-9.9 19.6-24.4 19.6z"/></symbol><symbol id="youtube" viewBox="0 0 576 512"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78.0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"/></symbol></svg><svg xmlns="http://www.w3.org/2000/svg" style="display:none"><symbol id="rss" viewBox="0 0 448 512"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328.0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765.0 183.105.0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686.0 38.981.0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"/></symbol></svg></head><body><header class=sk3-flex><nav class="sk3-sidebar box-right sk3-sidebar-trigger"><div></div><hr><div class="sk3-flex sk3-pad-h-full sk3-flex-center-h"><b>Share</b>
<sk3-social><a target=_blank rel="noopener noreferrer" href="//www.linkedin.com/shareArticle?mini=true&url=%2fhot-chips-31-live-blogs-cerebras-wafer-scale-deep-learning.html&title=Cerebras%27%201.2%20Trillion%20Transistor%20Deep%20Learning%20Processor" onclick='return window.open(this.href,"LinkedIn","width=640,height=480,toolbar=0,status=0"),!1'><svg class="sk3-social-fab"><use xlink:href="#linkedin"/></svg></a><a target=_blank rel="noopener noreferrer" href="//pinterest.com/pin/create/button/?url=%2fhot-chips-31-live-blogs-cerebras-wafer-scale-deep-learning.html&description=Cerebras%27%201.2%20Trillion%20Transistor%20Deep%20Learning%20Processor" onclick='return window.open(this.href,"Pinterest","width=800,height=720,toolbar=0,status=0"),!1'><svg class="sk3-social-fab"><use xlink:href="#pinterest"/></svg></a><a target=_blank rel="noopener noreferrer" href="//www.reddit.com/submit?url=%2fhot-chips-31-live-blogs-cerebras-wafer-scale-deep-learning.html&title=Cerebras%27%201.2%20Trillion%20Transistor%20Deep%20Learning%20Processor" onclick='return window.open(this.href,"Reddit","width=832,height=624,toolbar=0,status=0"),!1'><svg class="sk3-social-fab"><use xlink:href="#reddit"/></svg></a><a target=_blank rel="noopener noreferrer" href="//t.me/share/url?url=%2fhot-chips-31-live-blogs-cerebras-wafer-scale-deep-learning.html&title=Cerebras%27%201.2%20Trillion%20Transistor%20Deep%20Learning%20Processor" onclick='return window.open(this.href,"Telegram","width=800,height=600,toolbar=0,status=0"),!1'><svg class="sk3-social-fab"><use xlink:href="#telegram"/></svg></a><a target=_blank rel="noopener noreferrer" href="//twitter.com/intent/tweet/?url=%2fhot-chips-31-live-blogs-cerebras-wafer-scale-deep-learning.html&text=Cerebras%27%201.2%20Trillion%20Transistor%20Deep%20Learning%20Processor" onclick='return window.open(this.href,"Twitter","width=800,height=450,resizable=yes,toolbar=0,status=0"),!1'><svg class="sk3-social-fab"><use xlink:href="#twitter"/></svg></a><a target=_blank rel="noopener noreferrer" href="//vk.com/share.php?url=%2fhot-chips-31-live-blogs-cerebras-wafer-scale-deep-learning.html" onclick='return window.open(this.href,"VK","width=640,height=480,toolbar=0,status=0"),!1'><svg class="sk3-social-fab"><use xlink:href="#vk"/></svg></a></sk3-social></div><hr><div class="sk3-flex sk3-pad-h-full sk3-flex-center-h"><b>Follow</b>
<sk3-social><a href=//www.facebook.com//><svg class="sk3-social-fab"><use xlink:href="#facebook"/></svg></a><a href=//www.flickr.com/photos//><svg class="sk3-social-fab"><use xlink:href="#flickr"/></svg></a><a href=//github.com//><svg class="sk3-social-fab"><use xlink:href="#github"/></svg></a><a href=//www.instagram.com//><svg class="sk3-social-fab"><use xlink:href="#instagram"/></svg></a><a href=//www.linkedin.com/in//><svg class="sk3-social-fab"><use xlink:href="#linkedin"/></svg></a><a href=//www.pinterest.com//boards/><svg class="sk3-social-fab"><use xlink:href="#pinterest"/></svg></a><a href=//www.reddit.com/user/><svg class="sk3-social-fab"><use xlink:href="#reddit"/></svg></a><a href=//stackexchange.com/users//><svg class="sk3-social-fab"><use xlink:href="#stack-exchange"/></svg></a><a href=//stackoverflow.com/users//><svg class="sk3-social-fab"><use xlink:href="#stack-overflow"/></svg></a><a href=//www.tumblr.com/dashboard/blog//><svg class="sk3-social-fab"><use xlink:href="#tumblr"/></svg></a><a href=//twitter.com/><svg class="sk3-social-fab"><use xlink:href="#twitter"/></svg></a><a href=//vimeo.com/><svg class="sk3-social-fab"><use xlink:href="#vimeo"/></svg></a><a href=//www.youtube.com/user/><svg class="sk3-social-fab"><use xlink:href="#youtube"/></svg></a><a href=//www.youtube.com/channel/><svg class="sk3-social-fab"><use xlink:href="#youtube"/></svg></a><a href=./index.xml><svg class="sk3-social-fab"><use xlink:href="#rss"/></svg></a></sk3-social></div><hr></nav><div class="sk3-sidebar-mask sk3-sidebar-btn sk3-sidebar-trigger"></div><div class=sk3-flex><div class=sk3-sidebar-btn>☰</div><a class="sk3-title-pad-left sk3-margin-r" href=./index.html><h1>SnapJolt</h1></a></div></header><div id=content><div class=main><div></div><heading><h2>Cerebras' 1.2 Trillion Transistor Deep Learning Processor</h2></heading><div></div><content class=sk3-margin-v-full><p><a href=# id=post0819204952><span class=lb_time>08:49PM EDT</span></a> - Some of the big news of today is Cerebras announcing its wafer-scale 1.2 trillion transistor solution for deep learning. The talk today goes into detail about the technology.</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175010_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205102><span class=lb_time>08:51PM EDT</span></a> - Wafer scale chip, over 46,225 mm2, 1.2 trillion transistors, 400k AI cores, fed by 18GB of on-chip SRAM</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175016_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205119><span class=lb_time>08:51PM EDT</span></a> - TSMC 16nm</p><p><a href=# id=post0819205137><span class=lb_time>08:51PM EDT</span></a> - 215mm x 215mm - 8.5 inches per side</p><p><a href=# id=post0819205152><span class=lb_time>08:51PM EDT</span></a> - 56 times larger than the largest GPU today</p><p><a href=# id=post0819205227><span class=lb_time>08:52PM EDT</span></a> - Built for Deep Learning</p><p><a href=# id=post0819205231><span class=lb_time>08:52PM EDT</span></a> - DL training is hard (ed: this is an understatement)</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175210_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205255><span class=lb_time>08:52PM EDT</span></a> - Peta-to-exa scale compute range</p><p><a href=# id=post0819205302><span class=lb_time>08:53PM EDT</span></a> - The shape of the problem is difficult to scale</p><p><a href=# id=post0819205311><span class=lb_time>08:53PM EDT</span></a> - Fine grain has a lot of parallelism</p><p><a href=# id=post0819205317><span class=lb_time>08:53PM EDT</span></a> - Coarse grain is inherently serial</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175236_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205327><span class=lb_time>08:53PM EDT</span></a> - Training is the process of applying small changes, serially</p><p><a href=# id=post0819205337><span class=lb_time>08:53PM EDT</span></a> - Size and shape of the problem makes training NN really hard</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175342_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205358><span class=lb_time>08:53PM EDT</span></a> - Today we have dense vector compute</p><p><a href=# id=post0819205416><span class=lb_time>08:54PM EDT</span></a> - For Coarse Grained, require high speed interconnect to run mutliple instances. Still limited</p><p><a href=# id=post0819205422><span class=lb_time>08:54PM EDT</span></a> - Scaling is limited and costly</p><p><a href=# id=post0819205455><span class=lb_time>08:54PM EDT</span></a> - Specialized accelerators are the answer</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175439_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205504><span class=lb_time>08:55PM EDT</span></a> - NN: what is the right architecture</p><p><a href=# id=post0819205528><span class=lb_time>08:55PM EDT</span></a> - Need a core to be optimized for NN primitives</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175510_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205532><span class=lb_time>08:55PM EDT</span></a> - Need a programmable NN core</p><p><a href=# id=post0819205538><span class=lb_time>08:55PM EDT</span></a> - Needs to do sparse compute fast</p><p><a href=# id=post0819205542><span class=lb_time>08:55PM EDT</span></a> - Needs fast local memory</p><p><a href=# id=post0819205552><span class=lb_time>08:55PM EDT</span></a> - All of the cores should be connected with a fast interconnect</p><p><a href=# id=post0819205620><span class=lb_time>08:56PM EDT</span></a> - Cerebras uses flexible cores. Flexible general ops for control processing</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175559_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205629><span class=lb_time>08:56PM EDT</span></a> - Core should handle tensor operations very efficiency</p><p><a href=# id=post0819205637><span class=lb_time>08:56PM EDT</span></a> - Forms the bulk fo the compute in any neural network</p><p><a href=# id=post0819205645><span class=lb_time>08:56PM EDT</span></a> - Tensors as first class operands</p><p><a href=# id=post0819205720><span class=lb_time>08:57PM EDT</span></a> - fmac native op</p><p><a href=# id=post0819205758><span class=lb_time>08:57PM EDT</span></a> - NN naturally creates sparse networks</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175738_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205810><span class=lb_time>08:58PM EDT</span></a> - The core has native sparse processing in the hardware with dataflow scheduling</p><p><a href=# id=post0819205815><span class=lb_time>08:58PM EDT</span></a> - All the compute is triggered by the data</p><p><a href=# id=post0819205823><span class=lb_time>08:58PM EDT</span></a> - Filters all the sparse zeros, and filters the work</p><p><a href=# id=post0819205838><span class=lb_time>08:58PM EDT</span></a> - saves the power and energy, and get performance and acceleration by moving onto the next useful work</p><p><a href=# id=post0819205848><span class=lb_time>08:58PM EDT</span></a> - Enabled because arch has fine-grained execution datapaths</p><p><a href=# id=post0819205855><span class=lb_time>08:58PM EDT</span></a> - Many small cores with independent instructions</p><p><a href=# id=post0819205901><span class=lb_time>08:59PM EDT</span></a> - Allows for very non-uniform work</p><p><a href=# id=post0819205905><span class=lb_time>08:59PM EDT</span></a> - Next is memory</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_175910_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819205934><span class=lb_time>08:59PM EDT</span></a> - Traditional memory architectures are not optimized for DL</p><p><a href=# id=post0819205946><span class=lb_time>08:59PM EDT</span></a> - Traditional memory requires high data reuse for performane</p><p><a href=# id=post0819210004><span class=lb_time>09:00PM EDT</span></a> - Normal matrix multiply has low end data reuse</p><p><a href=# id=post0819210028><span class=lb_time>09:00PM EDT</span></a> - Translating Mat*Vec into Mat*Mat, but changes the training dynamics</p><p><a href=# id=post0819210044><span class=lb_time>09:00PM EDT</span></a> - Cerebras has high-perf, fully distributed on-chip SRAM next to the cores</p><p><a href=# id=post0819210106><span class=lb_time>09:01PM EDT</span></a> - Getting orders of magnitude higher bandwidth</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180051_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210120><span class=lb_time>09:01PM EDT</span></a> - ML can be done the way it wants to be done</p><p><a href=# id=post0819210141><span class=lb_time>09:01PM EDT</span></a> - High bandwidth, low latency interconnect</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180126_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210147><span class=lb_time>09:01PM EDT</span></a> - fast and fully configurable fabric</p><p><a href=# id=post0819210158><span class=lb_time>09:01PM EDT</span></a> - all hw based communication avoicd sw overhead</p><p><a href=# id=post0819210205><span class=lb_time>09:02PM EDT</span></a> - 2D mesh topology</p><p><a href=# id=post0819210218><span class=lb_time>09:02PM EDT</span></a> - higher utlization and efficiency than global topologies</p><p><a href=# id=post0819210248><span class=lb_time>09:02PM EDT</span></a> - Need more than a single die</p><p><a href=# id=post0819210254><span class=lb_time>09:02PM EDT</span></a> - Solition is a wafer scale</p><p><a href=# id=post0819210315><span class=lb_time>09:03PM EDT</span></a> - Build Big chips</p><p><a href=# id=post0819210321><span class=lb_time>09:03PM EDT</span></a> - Cluster scale perf on a single chip</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180258_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210330><span class=lb_time>09:03PM EDT</span></a> - GB of fast memory (SRAM) 1 clock cycle from the core</p><p><a href=# id=post0819210336><span class=lb_time>09:03PM EDT</span></a> - That's impossible with off-chip memory</p><p><a href=# id=post0819210343><span class=lb_time>09:03PM EDT</span></a> - Full on-chip interconnect fabric</p><p><a href=# id=post0819210356><span class=lb_time>09:03PM EDT</span></a> - Model parallel, linear performance scaling</p><p><a href=# id=post0819210406><span class=lb_time>09:04PM EDT</span></a> - Map the entire neural network onto the chip at once</p><p><a href=# id=post0819210418><span class=lb_time>09:04PM EDT</span></a> - One instance of NN, don't have to increase batch size to get cluster scale perf</p><p><a href=# id=post0819210425><span class=lb_time>09:04PM EDT</span></a> - Vastly lower power and less space</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180435_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210457><span class=lb_time>09:04PM EDT</span></a> - Can use TensorFlow and PyTorch</p><p><a href=# id=post0819210512><span class=lb_time>09:05PM EDT</span></a> - Performs placing and routing to map neural network layers to fabric</p><p><a href=# id=post0819210522><span class=lb_time>09:05PM EDT</span></a> - Entire wafer operates on the single neural network</p><p><a href=# id=post0819210529><span class=lb_time>09:05PM EDT</span></a> - Challenges of wafer scale</p><p><a href=# id=post0819210557><span class=lb_time>09:05PM EDT</span></a> - Need cross-die connectivity, yield, thermal expansion</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180534_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180630_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210655><span class=lb_time>09:06PM EDT</span></a> - Scribe line separates the die. On top of the scribe line, create wires</p><p><a href=# id=post0819210702><span class=lb_time>09:07PM EDT</span></a> - Extends 2D mesh fabric across all die</p><p><a href=# id=post0819210711><span class=lb_time>09:07PM EDT</span></a> - Same connectivity between cores and between die</p><p><a href=# id=post0819210732><span class=lb_time>09:07PM EDT</span></a> - More efficient than off-chip</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180719_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210747><span class=lb_time>09:07PM EDT</span></a> - Full BW at the die level</p><p><a href=# id=post0819210810><span class=lb_time>09:08PM EDT</span></a> - Redundancy helps yield</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180753_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210831><span class=lb_time>09:08PM EDT</span></a> - Redundant cores and redundant fabric links</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180816_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210848><span class=lb_time>09:08PM EDT</span></a> - Reconnect the fabric with links</p><p><a href=# id=post0819210855><span class=lb_time>09:08PM EDT</span></a> - Drive yields high</p><p><a href=# id=post0819210902><span class=lb_time>09:09PM EDT</span></a> - Transparent to software</p><p><a href=# id=post0819210924><span class=lb_time>09:09PM EDT</span></a> - Thermal expansion</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_180909_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819210935><span class=lb_time>09:09PM EDT</span></a> - Normal tech, too much mechanical stress via thermal expansion</p><p><a href=# id=post0819210939><span class=lb_time>09:09PM EDT</span></a> - Custom connector developed</p><p><a href=# id=post0819210948><span class=lb_time>09:09PM EDT</span></a> - Connector absorbs the variation in thermal expansion</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181005_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819211026><span class=lb_time>09:10PM EDT</span></a> - All components need to be held with precise alignment - custom packaging tools</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181031_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819211049><span class=lb_time>09:10PM EDT</span></a> - Power and Cooling</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181056_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819211117><span class=lb_time>09:11PM EDT</span></a> - Power planes don't work - isn't enough copper in the PCB to do it that way</p><p><a href=# id=post0819211128><span class=lb_time>09:11PM EDT</span></a> - Heat density too high for direct air cooling</p><p><a href=# id=post0819211206><span class=lb_time>09:12PM EDT</span></a> - Bring current perpendicular to the wafer. Water cooled perpendicular too</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181142_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181224_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819211406><span class=lb_time>09:14PM EDT</span></a> - Q&A Time</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/14758/IMG_20190819_181341_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=# id=post0819211445><span class=lb_time>09:14PM EDT</span></a> - Q and A</p><p><a href=# id=post0819211458><span class=lb_time>09:14PM EDT</span></a> - Already in use? Yes</p><p><a href=# id=post0819211515><span class=lb_time>09:15PM EDT</span></a> - Can you make a round chip? Square is more convenient</p><p><a href=# id=post0819211558><span class=lb_time>09:15PM EDT</span></a> - Yield? Mature processes are quite good and uniform</p><p><a href=# id=post0819211658><span class=lb_time>09:16PM EDT</span></a> - Does it cost less than a house? Everything is amortised across the wafer</p><p><a href=# id=post0819211722><span class=lb_time>09:17PM EDT</span></a> - Regular processor for housekeeping? They can all do it</p><p><a href=# id=post0819211741><span class=lb_time>09:17PM EDT</span></a> - Is it fully synchronous? No</p><p><a href=# id=post0819212001><span class=lb_time>09:20PM EDT</span></a> - Clock rate? Not disclosed</p><p><a href=# id=post0819212042><span class=lb_time>09:20PM EDT</span></a> - That's a wrap. Next is Habana</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7orrAp5utnZOde6S7zGiqoaenZH51g5RxZqGnpGKwqbXPrGRsaV2htrexjJujqJ%2BjYrCmvsSbqZqrXayup7HRZqqcmZyaeqWxxKlkpZ2Rp7uqusY%3D</p></content><div></div><section class=sk3-margin-v-full><section class="sk3-flex sk3-flex-center-h"><div class="sk3-card sk3-flex sk3-flex-center-h box"><div class="sk3-flex sk3-flex-center-v sk3-flex-center-h"><sk3-social><a target=_blank rel="noopener noreferrer" href="//www.linkedin.com/shareArticle?mini=true&url=%2fhot-chips-31-live-blogs-cerebras-wafer-scale-deep-learning.html&title=Cerebras%27%201.2%20Trillion%20Transistor%20Deep%20Learning%20Processor" onclick='return window.open(this.href,"LinkedIn","width=640,height=480,toolbar=0,status=0"),!1'><svg class="sk3-social-fab"><use xlink:href="#linkedin"/></svg></a><a target=_blank rel="noopener noreferrer" href="//pinterest.com/pin/create/button/?url=%2fhot-chips-31-live-blogs-cerebras-wafer-scale-deep-learning.html&description=Cerebras%27%201.2%20Trillion%20Transistor%20Deep%20Learning%20Processor" onclick='return window.open(this.href,"Pinterest","width=800,height=720,toolbar=0,status=0"),!1'><svg class="sk3-social-fab"><use xlink:href="#pinterest"/></svg></a><a target=_blank rel="noopener noreferrer" href="//www.reddit.com/submit?url=%2fhot-chips-31-live-blogs-cerebras-wafer-scale-deep-learning.html&title=Cerebras%27%201.2%20Trillion%20Transistor%20Deep%20Learning%20Processor" onclick='return window.open(this.href,"Reddit","width=832,height=624,toolbar=0,status=0"),!1'><svg class="sk3-social-fab"><use xlink:href="#reddit"/></svg></a><a target=_blank rel="noopener noreferrer" href="//t.me/share/url?url=%2fhot-chips-31-live-blogs-cerebras-wafer-scale-deep-learning.html&title=Cerebras%27%201.2%20Trillion%20Transistor%20Deep%20Learning%20Processor" onclick='return window.open(this.href,"Telegram","width=800,height=600,toolbar=0,status=0"),!1'><svg class="sk3-social-fab"><use xlink:href="#telegram"/></svg></a><a target=_blank rel="noopener noreferrer" href="//twitter.com/intent/tweet/?url=%2fhot-chips-31-live-blogs-cerebras-wafer-scale-deep-learning.html&text=Cerebras%27%201.2%20Trillion%20Transistor%20Deep%20Learning%20Processor" onclick='return window.open(this.href,"Twitter","width=800,height=450,resizable=yes,toolbar=0,status=0"),!1'><svg class="sk3-social-fab"><use xlink:href="#twitter"/></svg></a><a target=_blank rel="noopener noreferrer" href="//vk.com/share.php?url=%2fhot-chips-31-live-blogs-cerebras-wafer-scale-deep-learning.html" onclick='return window.open(this.href,"VK","width=640,height=480,toolbar=0,status=0"),!1'><svg class="sk3-social-fab"><use xlink:href="#vk"/></svg></a></sk3-social></div></div><div class="sk3-card sk3-flex sk3-flex-center-h box"><div class="sk3-flex sk3-flex-center-v sk3-flex-center-h"><div class=sk3-margin-h><h3>Martina Birk</h3></div><div class="sk3-margin-h sk3-margin-down">Update: 2024-02-12</div></div></div><div class="sk3-card sk3-flex sk3-flex-center-h box"><div class="sk3-flex sk3-flex-center-v sk3-flex-center-h"><a class="sk3-margin-h txt-center-h" href=./clay-matthews-iii.html><h3>&lt;&lt; Clay Matthews Iii Biography, Facts &amp;amp; Life Story</h3></a><a class="sk3-margin-h txt-center-h" href=./beckah-shae.html><h3>Beckah Shae >></h3></a></div></div></section></section><div></div></div></div><footer class="sk3-flex sk3-flex-center-h sk3-margin-v-full sk3-pad-v-full"><div class="sk3-pad-h txt-center-h">©
2012-2024 <a href=./>SnapJolt</a>.</div><div class="sk3-pad-h txt-center-h">Power by <a href=//gohugo.io>Hugo</a>.</div></footer><meta property="og:title" content="Cerebras' 1.2 Trillion Transistor Deep Learning Processor"><meta property="og:description" content="08:49PM EDT - Some of the big news of today is Cerebras announcing its wafer-scale 1.2 trillion transistor solution for deep learning. The talk today goes into detail about the technology. 08:51PM EDT - Wafer scale chip, over 46,225 mm2, 1.2 trillion transistors, 400k AI cores, fed by 18GB of on-chip SRAM"><meta property="og:type" content="article"><meta property="og:url" content="/hot-chips-31-live-blogs-cerebras-wafer-scale-deep-learning.html"><meta property="article:section" content="post"><meta property="article:published_time" content="2024-02-12T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-12T00:00:00+00:00"><meta property="og:site_name" content="SnapJolts"><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/floating.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>